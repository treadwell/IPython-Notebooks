{
 "metadata": {
  "name": "CSA Data Science Sentiment Analysis"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# twitterstream.py\n",
      "\n",
      "import oauth2 as oauth\n",
      "import urllib2 as urllib\n",
      "\n",
      "# See Assginment 6 instructions or README for how to get these credentials\n",
      "access_token_key = \"17865318-vazH88XQqhNfCWs1Z0AOO1VnGwSeqRxM9pSLXOMDj\"\n",
      "access_token_secret = \"sCyWjIPkF4B5yti5rh0f3lHXv8Rcvd0iHcg3fEfUU\"\n",
      "\n",
      "consumer_key = \"Cn4VnweHb78b280ofZrog\"\n",
      "consumer_secret = \"iYsWRy0ODpcZwsJdPucouZ9drBjpSiBQq4MIX6h36Y\"\n",
      "\n",
      "_debug = 0\n",
      "\n",
      "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
      "oauth_consumer = oauth.Consumer(key=consumer_key, secret=consumer_secret)\n",
      "\n",
      "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
      "\n",
      "http_method = \"GET\"\n",
      "\n",
      "\n",
      "http_handler  = urllib.HTTPHandler(debuglevel=_debug)\n",
      "https_handler = urllib.HTTPSHandler(debuglevel=_debug)\n",
      "\n",
      "'''\n",
      "Construct, sign, and open a twitter request\n",
      "using the hard-coded credentials above.\n",
      "'''\n",
      "def twitterreq(url, method, parameters):\n",
      "  req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
      "                                             token=oauth_token,\n",
      "                                             http_method=http_method,\n",
      "                                             http_url=url, \n",
      "                                             parameters=parameters)\n",
      "\n",
      "  req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
      "\n",
      "  headers = req.to_header()\n",
      "\n",
      "  if http_method == \"POST\":\n",
      "    encoded_post_data = req.to_postdata()\n",
      "  else:\n",
      "    encoded_post_data = None\n",
      "    url = req.to_url()\n",
      "\n",
      "  opener = urllib.OpenerDirector()\n",
      "  opener.add_handler(http_handler)\n",
      "  opener.add_handler(https_handler)\n",
      "\n",
      "  response = opener.open(url, encoded_post_data)\n",
      "\n",
      "  return response\n",
      "\n",
      "def fetchsamples():\n",
      "  url = \"https://stream.twitter.com/1/statuses/sample.json\"\n",
      "  parameters = []\n",
      "  response = twitterreq(url, \"GET\", parameters)\n",
      "  for line in response:\n",
      "    print line.strip()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  fetchsamples()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named oauth2",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-73c175eba03e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0moauth2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# See Assginment 6 instructions or README for how to get these credentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccess_token_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"17865318-vazH88XQqhNfCWs1Z0AOO1VnGwSeqRxM9pSLXOMDj\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named oauth2"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# term_sentiment.py\n",
      "\n",
      "import json\n",
      "import sys\n",
      "\n",
      "def hw(sent_file, tweet_file):\n",
      "    # Load sentiment file into a dictionary\n",
      "    sent_dict = {}\n",
      "    for line in sent_file:\n",
      "\tsentiment_pair = line.split('\\t')\n",
      "\tsent_dict[sentiment_pair[0]]=sentiment_pair[1]\n",
      "\n",
      "    # parse tweet file down to words in tweets\n",
      "\n",
      "    data = []\n",
      "    new_terms = {}  # initialize new terms dictionary\n",
      "    for line in tweet_file:\n",
      "        line_data = json.loads(line)\n",
      "        lang = line_data.get(\"lang\")\n",
      "        # utf unencode tweet below, if necessary\n",
      "\ttweet = line_data.get(\"text\")\n",
      "        if lang == \"en\":  # limit tweets to english\n",
      "\t    sum = 0\n",
      "\t    count = 0\n",
      "\t    for word in tweet.split():\n",
      "\t\tif word in sent_dict:\n",
      "\t\t    sum += int(sent_dict[word])\n",
      "                else:\n",
      "\t\t    sum += 0\n",
      "\t\tcount += 1\n",
      "\t    tweet_score = sum\n",
      "\t    for word in tweet.split():\n",
      "\t\tif word not in sent_dict:\n",
      "\t\t    if word not in new_terms:\n",
      "\t\t        # add it with [pos_count += 1, neg_count += 1, total_count, weight]\n",
      "\t\t        if tweet_score > 0:\n",
      "\t\t\t    new_terms[word] = [1,0,1,1]\n",
      "\t\t        elif tweet_score < 0:\n",
      "\t\t\t    new_terms[word] = [0,1,1,-1]\n",
      "\t\t        else:\n",
      "\t\t\t   new_terms[word] = [0,0,1,0]\n",
      "\t\t    if word in new_terms:\n",
      "\t\t        # increment [pos_count += 1, neg_count += 1, total_count, weight]\n",
      "\t\t        if tweet_score > 0:\n",
      "\t\t\t    new_terms[word][0] += 1.0\n",
      "\t\t\t    new_terms[word][2] += 1.0\n",
      "\t\t\t    new_terms[word][3] = (new_terms[word][0] - new_terms[word][1]) / new_terms[word][2]\n",
      "\t\t        elif tweet_score < 0:\n",
      "\t\t\t    new_terms[word][2] += 1.0\n",
      "\t\t\t    new_terms[word][2] += 1.0\n",
      "\t\t\t    new_terms[word][3] = (new_terms[word][0] - new_terms[word][1]) / new_terms[word][2]\n",
      "\t\t        else:\n",
      "\t\t\t    new_terms[word][2] += 1.0\n",
      "\t\t\t    new_terms[word][3] = (new_terms[word][0] - new_terms[word][1]) / new_terms[word][2]\n",
      "    for term in new_terms.keys():\n",
      "\tprint \"%s %.3f\"%(term.encode('utf-8'),new_terms[term][3])\n",
      "\n",
      "def lines(fp):\n",
      "    print str(len(fp.readlines()))\n",
      "\n",
      "def main():\n",
      "    sent_file = open(sys.argv[1])\n",
      "    tweet_file = open(sys.argv[2])\n",
      "    hw(sent_file, tweet_file)\n",
      "    #lines(sent_file)\n",
      "    #lines(tweet_file)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tweet_sentiment.py\n",
      "\n",
      "import json\n",
      "import sys\n",
      "\n",
      "def hw(sent_file, tweet_file):\n",
      "    # Load sentiment file into a dictionary\n",
      "    sent_dict = {}\n",
      "    for line in sent_file:\n",
      "\tsentiment_pair = line.split('\\t')\n",
      "\tsent_dict[sentiment_pair[0]]=sentiment_pair[1]\n",
      "    #for entry in sent_dict:\n",
      "\t#print entry, sent_dict[entry]\n",
      "    # parse tweet file down to words in tweets\n",
      "\n",
      "    data = []\n",
      "    new_terms = {}  # initialize new terms dictionary\n",
      "    for line in tweet_file:\n",
      "        line_data = json.loads(line)\n",
      "        lang = line_data.get(\"lang\")\n",
      "        # utf unencode tweet below, if necessary\n",
      "\ttweet = line_data.get(\"text\")\n",
      "        if tweet:  # if lang == \"en\" limits tweets to english\n",
      "\t    sum = 0\n",
      "\t    count = 0\n",
      "\t    for word in tweet.split():\n",
      "\t\tif word in sent_dict:\n",
      "\t\t    sum += int(sent_dict[word])\n",
      "                else:\n",
      "\t\t    sum += 0\n",
      "\t\tcount += 1\n",
      "\t    tweet_score = sum\n",
      "\t    print \"%.3f\"%(tweet_score)\n",
      "\t\t\n",
      "def lines(fp):\n",
      "    print str(len(fp.readlines()))\n",
      "\n",
      "def main():\n",
      "    sent_file = open(sys.argv[1])\n",
      "    tweet_file = open(sys.argv[2])\n",
      "    hw(sent_file, tweet_file)\n",
      "    #lines(sent_file)\n",
      "    #lines(tweet_file)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# frequency.py\n",
      "\n",
      "import json\n",
      "import sys\n",
      "\n",
      "def hw(tweet_file):\n",
      "\n",
      "    # parse tweet file down to words in tweets\n",
      "\n",
      "    data = []\n",
      "    new_terms = {}  # initialize new terms dictionary\n",
      "    total_term_occurrences = 0.0  #  initialize the number of occurrences of all terms in all tweets\n",
      "    for line in tweet_file:\n",
      "        line_data = json.loads(line)\n",
      "        lang = line_data.get(\"lang\")\n",
      "\ttweet = line_data.get(\"text\")\n",
      "        if tweet:  # change this to limit tweets to English, if necessary\n",
      "\t    for word in tweet.split():\n",
      "\t\tif word not in new_terms:\n",
      "                    # increment total_term_occurrences\n",
      "\t\t    total_term_occurrences += 1\n",
      "\t\t    # add it with count = 1\n",
      "                    new_terms[word] = 1.0\n",
      "\t\telse:\n",
      "                    # increment total_term_occurrences\n",
      "\t\t    total_term_occurrences += 1\n",
      "\t\t    # increment count (count += 1)\n",
      "                    new_terms[word] += 1\n",
      "    # print terms and frequencies\n",
      "    for term in new_terms.keys():\n",
      "\tprint \"%s %.5f\"%(term.encode('utf-8'), float(new_terms[term]/total_term_occurrences))\n",
      "\n",
      "def lines(fp):\n",
      "    print str(len(fp.readlines()))\n",
      "\n",
      "def main():\n",
      "    tweet_file = open(sys.argv[1])\n",
      "    hw(tweet_file)\n",
      "    #lines(tweet_file)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# happiest_state.py\n",
      "\n",
      "import json\n",
      "import sys\n",
      "import re\n",
      "\n",
      "def hw(sent_file, tweet_file):\n",
      "    # Load sentiment file into a dictionary\n",
      "    sent_dict = {}\n",
      "    for line in sent_file:\n",
      "\tsentiment_pair = line.split('\\t')\n",
      "\tsent_dict[sentiment_pair[0]]=sentiment_pair[1]\n",
      "    #for entry in sent_dict:\n",
      "\t#print entry, sent_dict[entry]\n",
      "    # parse tweet file down to words in tweets\n",
      "\n",
      "    data = []\n",
      "    state_sentiment = {}\n",
      "    \t\n",
      "    for line in tweet_file:\n",
      "        line_data = json.loads(line)\n",
      "        lang = line_data.get(\"lang\")\n",
      "\tplace = line_data.get(\"place\")\n",
      "\ttweet = line_data.get(\"text\")\n",
      "\tcount = 0\n",
      "        tweet_count = 0\n",
      "\t#country = place[\"country_code\"]\n",
      "        if place and place[\"country_code\"] and place[\"full_name\"]:\n",
      "            if place[\"country_code\"] == \"US\" and re.search(\"[A-Z][A-Z]$\", place[\"full_name\"][-2:]):\n",
      "\t        state = place[\"full_name\"][-2:]\n",
      "\t\tsum = 0\n",
      "\t        tweet_count += 1\n",
      "\t        for word in tweet.split():\n",
      "\t\t    if word in sent_dict:\n",
      "\t\t        sum += float(sent_dict[word])\n",
      "                    else:\n",
      "\t\t        sum += 0\n",
      "\t        tweet_score = sum\n",
      "\t\t# state_sentiment has structure [tweet count, total tweet score, average tweet score]\n",
      "\t\tif state not in state_sentiment.keys():\n",
      "\t\t    state_sentiment[state] = [1, tweet_score, tweet_score]\n",
      "\t\telse:\n",
      "\t\t    state_sentiment[state][0] += 1\n",
      "\t\t    state_sentiment[state][1] += tweet_score\n",
      "\t\t    state_sentiment[state][2] = state_sentiment[state][1] / state_sentiment[state][0]\n",
      "\t\t#print state, state_sentiment[state][0], tweet_score, state_sentiment[state][1], state_sentiment[state][2]\n",
      "    #print  \"-----------------------------------\"\n",
      "    happiest_state = \"\"\n",
      "    happiest_score = -99\n",
      "    for state in state_sentiment.keys():\n",
      "\tif state_sentiment[state][2] > happiest_score:\n",
      "\t    happiest_state = state\n",
      "            happiest_score = state_sentiment[state][2]\n",
      "        #print state, state_sentiment[state][0], state_sentiment[state][2]\n",
      "    #print  \"-----------------------------------\"\n",
      "    print happiest_state\n",
      "\n",
      "\t\t\n",
      "def lines(fp):\n",
      "    print str(len(fp.readlines()))\n",
      "\n",
      "def main():\n",
      "    sent_file = open(sys.argv[1])\n",
      "    tweet_file = open(sys.argv[2])\n",
      "    hw(sent_file, tweet_file)\n",
      "    #lines(sent_file)\n",
      "    #lines(tweet_file)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# top_ten.py\n",
      "\n",
      "import json\n",
      "import sys\n",
      "\n",
      "\n",
      "def hw(tweet_file):\n",
      "\n",
      "    data = []\n",
      "    hashtag_dict = {}\n",
      "    for line in tweet_file:\n",
      "        line_data = json.loads(line)\n",
      "\tentities = line_data.get(\"entities\")\n",
      "        if entities and entities[\"hashtags\"]:\n",
      "\t    for hashtags in entities[\"hashtags\"]: #[0][\"text\"]\n",
      "\t\t#print hashtags[\"text\"]\n",
      "\t\tif hashtags[\"text\"] not in hashtag_dict.keys():\n",
      "\t\t    hashtag_dict[hashtags[\"text\"]] = 1\n",
      "\t\telse:\n",
      "\t\t    hashtag_dict[hashtags[\"text\"]] += 1\n",
      "    for hashtag in sorted(hashtag_dict, key=hashtag_dict.get, reverse=True)[:10]:\n",
      "\tprint hashtag, hashtag_dict[hashtag]\n",
      "\n",
      "\t\t\n",
      "def lines(fp):\n",
      "    print str(len(fp.readlines()))\n",
      "\n",
      "def main():\n",
      "    tweet_file = open(sys.argv[1])\n",
      "    hw(tweet_file)\n",
      "    #lines(tweet_file)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}